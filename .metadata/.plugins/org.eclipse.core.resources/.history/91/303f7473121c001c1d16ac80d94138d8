import findspark
from pyspark import SparkConf, SparkContext, SQLContext;


#from pyspark.sql.types import StringType;
#import numpy as np;
#===============================================================================
#
#===============================================================================
findspark.init();

confSpark= SparkConf().setMaster("local").setAppName("My_Program");
sc= SparkContext(conf= confSpark); 

lines= sc.textFile("C:/Users/Alejandro Molinar/Documents/ejemplo.txt");
#===============================================================================
# 
#===============================================================================

mapFunc= A.map(lambda x: (x,1));

#print(mapFunc.collect());


def funcion(x):
    if x[0] in ["SEA", "ATL", "HOU", "IND"]:
        
        return((x , 2));
    
    elif x[0] in ["DEN", "BOS"]:
        
        return((x , 3));
    
    else:
        return ((x , 1));
    
mapFunc2= A.map(funcion);

#print(mapFunc.collect());

reduceFunc= mapFunc.reduceByKey(lambda x,y: x+y); 

reduceFun2= mapFunc2.reduceByKey(lambda x,y: x+y); 
    
reduceFunc.sortBy(lambda x: x[1], ascending=False).show();
reduceFunc.sortByKey().show();
    
    